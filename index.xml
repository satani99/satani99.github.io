<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Nikhil&#39;s blog</title>
    <link>https://satani99.github.io/</link>
    <description>Recent content on Nikhil&#39;s blog</description>
    <generator>Hugo -- 0.123.7</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 14 Feb 2026 08:37:08 +0530</lastBuildDate>
    <atom:link href="https://satani99.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Converting an AR Code Model (Qwen-Coder-Next) into a Diffusion Language Model</title>
      <link>https://satani99.github.io/posts/converting-an-ar-code-model-qwen-coder-next-into-a-diffusion-language-model/</link>
      <pubDate>Sat, 14 Feb 2026 08:37:08 +0530</pubDate>
      <guid>https://satani99.github.io/posts/converting-an-ar-code-model-qwen-coder-next-into-a-diffusion-language-model/</guid>
      <description>Auto-regressive models like Qwen3-Coder-Next generate code token-by-token, but diffusion-based LMs (dLLMs) generate by denoising an entire sequence in parallel. Recent research suggests diffusion is very promising for code: Apple’s DiffuCoder (7B parameters) is a diffusion LLM fine-tuned on code tasks, built from the Qwen-2.5-Coder backbone【35†L252-L261】. It iteratively masks random tokens and learns to reconstruct them, enabling “global planning” of code rather than strict left-to-right generation【35†L252-L261】【5†L73-L82】. In practice, this can yield competitive or superior pass@k on benchmarks (DiffuCoder beats other code models on MBPP【35†L252-L261】).</description>
    </item>
    <item>
      <title>Decoupled DMD for Diffusion Language Models</title>
      <link>https://satani99.github.io/posts/decoupled-dmd-for-diffusion-language-models/</link>
      <pubDate>Tue, 20 Jan 2026 10:50:21 +0530</pubDate>
      <guid>https://satani99.github.io/posts/decoupled-dmd-for-diffusion-language-models/</guid>
      <description>Decoupled DMD for Diffusion Language Models: The Spear &amp;amp; The Shield</description>
    </item>
    <item>
      <title>Diffusion Model Triton Optimization</title>
      <link>https://satani99.github.io/posts/triton-in-diffusion-model/</link>
      <pubDate>Tue, 02 Dec 2025 14:08:05 +0530</pubDate>
      <guid>https://satani99.github.io/posts/triton-in-diffusion-model/</guid>
      <description>Accelerated Generative Dynamics: A Comparative Analysis of PyTorch and Custom Triton Kernels for Fused GroupNorm-SiLU Inference in Diffusion Models</description>
    </item>
    <item>
      <title>Diffusion Language Model</title>
      <link>https://satani99.github.io/posts/diffusion-language-model/</link>
      <pubDate>Thu, 20 Nov 2025 18:59:37 +0530</pubDate>
      <guid>https://satani99.github.io/posts/diffusion-language-model/</guid>
      <description>How Diffusion Language Models Work: Theory, Implementation, and Code</description>
    </item>
  </channel>
</rss>
